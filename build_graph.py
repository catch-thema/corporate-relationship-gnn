# NOTE:
# - í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜: OPENAI_API_KEY, DART_API_KEY
# - ì˜ì¡´ì„±ì€ requirements.txtì— ëª…ì‹œí•˜ì„¸ìš”.
#   ì˜ˆ: pip install -r requirements.txt
# - DART API ë¬¸ì„œ: https://opendart.fss.or.kr/guide/main.do

import os
import zipfile
import requests
from bs4 import BeautifulSoup
import re
import networkx as nx
import torch
from tqdm import tqdm
import pandas as pd
import xmltodict
import io
import json
import pickle  # Added

# --- Replace hardcoded API keys with environment variables ---
from openai import OpenAI

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DART_API_KEY = os.getenv("DART_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
if not DART_API_KEY:
    raise RuntimeError("DART_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

client = OpenAI(api_key=OPENAI_API_KEY)

def load_corp_code(api_key):
    url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={api_key}"

    res = requests.get(url)
    print("Content-Type:", res.headers.get("Content-Type"))

    z = zipfile.ZipFile(io.BytesIO(res.content))
    xml_name = z.namelist()[0]
    xml_data = z.read(xml_name)

    with open("CORPCODE.xml", "wb") as f:
        f.write(xml_data)
    print("XML ì €ì¥ ì™„ë£Œ: CORPCODE.xml")

    data = xmltodict.parse(xml_data)

    # DataFrame ìƒì„±
    corp_df = pd.DataFrame(data["result"]["list"])

    # ğŸ”¥ stock_code NOT NULL â†’ ìƒì¥ì‚¬ë§Œ ì¶”ì¶œ
    corp_df = corp_df[corp_df["stock_code"].notnull() & (corp_df["stock_code"] != "")]
    corp_df = corp_df.reset_index(drop=True)

    return corp_df

corp_list = load_corp_code(DART_API_KEY)
print("CORP list loaded:", corp_list.shape)

def get_latest_business_report(corp_code):
    # ë³€ê²½: API_KEY -> DART_API_KEY
    url = (
        "https://opendart.fss.or.kr/api/list.json"
        f"?crtfc_key={DART_API_KEY}&corp_code={corp_code}&bgn_de=20200101&pblntf_detail_ty=A001"
    )
    res = requests.get(url).json()

    if res.get("status") != "013" and res.get("list"):
        return res["list"][0]["rcept_no"]
    return None

def extract_html_from_document_zip(rcept_no):
    # ë³€ê²½: API_KEY -> DART_API_KEY
    url = f"https://opendart.fss.or.kr/api/document.xml?crtfc_key={DART_API_KEY}&rcept_no={rcept_no}"
    res = requests.get(url)

    content = res.content

    if content[:2] == b'PK':
        print("âœ” document.xml: ZIP íŒŒì¼ ê°ì§€")

        try:
            z = zipfile.ZipFile(io.BytesIO(content))
        except:
            raise Exception("âŒ ZIP íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨")

        print("ZIP ë‚´ë¶€:", z.namelist())

        # 1) ì‚¬ì—…ë³´ê³ ì„œ ë³¸ë¬¸ íŒŒì¼ ìš°ì„  ì„ íƒ
        main_file = f"{rcept_no}.xml"
        if main_file in z.namelist():
            raw = z.read(main_file)
            text = None
            for enc in ["utf-8", "euc-kr", "cp949"]:
                try:
                    text = raw.decode(enc)
                    break
                except:
                    pass

            if text is None:
                print(f"âŒ ë””ì½”ë”© ì‹¤íŒ¨: {main_file}")

            soup = BeautifulSoup(text, "html.parser")
            print(f"âœ” HTML íŒŒì‹± ì„±ê³µ: {main_file}")
            return soup

        for name in z.namelist():
          raw = z.read(name) # ë””ì½”ë”© ì‹œë„
          text = None
          for enc in ["utf-8", "euc-kr", "cp949"]:
              try:
                text = raw.decode(enc)
                break
              except:
                  pass

          if text is None:
              print(f"âŒ ë””ì½”ë”© ì‹¤íŒ¨: {name}")
              continue # BeautifulSoup ë¡œ HTML íŒŒì‹± ì‹œë„
          try:
            soup = BeautifulSoup(text, "html.parser")
            print(f"âœ” HTML íŒŒì‹± ì„±ê³µ: {name}")
            return soup
          except Exception as e:
            print(f"âŒ HTML íŒŒì‹± ì‹¤íŒ¨: {name}", e)
            continue
        return None

    else:
      # -------------------------------
      # â‘¡ ZIPì´ ì•„ë‹ˆë¼ ë‹¨ì¼ XML ë¬¸ì„œì¼ ë•Œ
      # -------------------------------
      print("âœ” document.xml: ë‹¨ì¼ XML ë¬¸ì„œ ê°ì§€")

      try:
          text = content.decode("utf-8")
      except:
          try:
              text = content.decode("euc-kr")
          except:
              try:
                  text = content.decode("cp949")
              except:
                  raise Exception("âŒ ë‹¨ì¼ XML ë””ì½”ë”© ì‹¤íŒ¨")

      soup = BeautifulSoup(text, "html.parser")
      print("âœ” ë‹¨ì¼ XML HTML íŒŒì‹± ì„±ê³µ")
      return soup

import re

SECTION_TITLES = [
    r'â… \.\s*íšŒì‚¬[ì˜]? ê°œìš”',   r'I\.\s*íšŒì‚¬[ì˜]? ê°œìš”',
    r'â…¡\.\s*ì‚¬ì—…[ì˜]? ë‚´ìš©',   r'II\.\s*ì‚¬ì—…[ì˜]? ë‚´ìš©',
    r'ê³„ì—´íšŒì‚¬\s*í˜„í™©',
    r'ì¢…ì†ê¸°ì—…[ì˜\s]*ê°œí™©',
    r'ê´€ê³„íšŒì‚¬\s*í˜„í™©',
]

SECTION_PATTERN = "(" + "|".join(SECTION_TITLES) + ")"

def split_sections(text):
    parts = re.split(SECTION_PATTERN, text)
    sections = {}
    current = None

    for p in parts:
        if re.match(SECTION_PATTERN, p):
            current = p
            sections[current] = ""
        else:
            if current:
                sections[current] += p + "\n"
    return sections

def extract_key_sections(text):
    all_sections = split_sections(text)
    result = {"business": "", "subsidiaries": ""}

    for title, body in all_sections.items():
        if re.search("ì‚¬ì—…[ì˜ ]*ë‚´ìš©", title):
            result["business"] += body
        if re.search("(ê³„ì—´íšŒì‚¬|ì¢…ì†ê¸°ì—…|ê´€ê³„íšŒì‚¬)", title):
            result["subsidiaries"] += body

    return result

RELATION_KEYWORDS = [
    # governance
    "ì¢…ì†", "ìíšŒì‚¬", "ê´€ê³„íšŒì‚¬", "ê³„ì—´íšŒì‚¬", "ê³„ì—´", "ì§€ë¶„", "ì§€ë°°", "í•©ì‘",
    "joint venture", "JV", "associate", "affiliate", "subsidiary",
    "ì§€ë°°êµ¬ì¡°", "ì¶œì",

    # supplier
    "ê³µê¸‰", "ë‚©í’ˆ", "ì›ì¬ë£Œ", "ì¬ë£Œê³µê¸‰", "ë¶€í’ˆê³µê¸‰", "supplier",

    # customer
    "ê³ ê°ì‚¬", "ë§¤ì¶œì²˜", "ì£¼ìš”ê³ ê°", "íŒë§¤ì²˜", "ìˆ˜ìš”ê¸°ì—…", "customer",

    # competitor
    "ê²½ìŸì‚¬", "ê²½ìŸê¸°ì—…", "ê²½ìŸì‚¬ë“¤", "ê²½ìŸ",

    # ê¸°ìˆ /í˜‘ë ¥
    "í˜‘ë ¥", "íŒŒíŠ¸ë„ˆ", "ê¸°ìˆ ì œíœ´", "ë¼ì´ì„ ìŠ¤", "license", "oem",
    "ê¸°ìˆ í˜‘ë ¥", "ê¸°ìˆ ê³µë™",

    # ê¸ˆìœµ/íˆ¬ì
    "íˆ¬ì", "ì§€ë¶„ì°¸ì—¬", "ì¶œì", "í€ë“œ", "loan", "ê¸ˆìœµì§€ì›", "underwriter",

    # ìœ í†µ
    "ìœ í†µ", "ë¬¼ë¥˜", "ë°°ì†¡", "logistics", "distribution partner",
]

NEGATIVE_PATTERNS = [
    "ì„¤ëª…", "ê¸°ì¤€", "ì‘ì„±", "ì°¸ê³ ", "ëª©ì ", "ìš”ì•½",
    "ì¬ë¬´ì œí‘œ", "íšŒê³„", "ê°ì‚¬", "ë²•ë ¹", "ê´€ë ¨ ê·œì •",
    "ê¸°ì¤€ì¼", "ê³µì‹œ", "ë³´ê³ ì„œ", "ì´ê´„", "ê°œìš”", "ì¼ë°˜ì‚¬í•­"
]

def split_sentences(text):
    # ì (.) , ë‹¤. , ? , ! , \n ë¡œ êµ¬ë¶„
    pattern = r'(?<=[\.!?])\s+|(?<=ë‹¤\.)\s+|\n+'
    s_list = re.split(pattern, text)
    return [s.strip() for s in s_list if len(s.strip()) > 8]


def extract_relation_sentences(text):
    sentences = split_sentences(text)
    result = []

    for s in sentences:
        # ê´€ê³„ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        if not any(k in s for k in RELATION_KEYWORDS):
            continue

        # ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±°
        if any(n in s for n in NEGATIVE_PATTERNS):
            continue

        # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
        if len(s) < 15:
            continue

        result.append(s.strip())

    return result

from bs4 import BeautifulSoup

def extract_subsidiary_table(soup):
    """
    DART OOXML ê¸°ë°˜ HTML/XMLì—ì„œ ì¢…ì†/ê³„ì—´íšŒì‚¬ í…Œì´ë¸”ì„ robustí•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ìµœì¢…ë³¸
    """

    # í‘œ í›„ë³´ ì „ì²´ ìˆ˜ì§‘
    tables = soup.find_all(["table", "TABLE"])
    subsidiaries = []

    if not tables:
        print("âš ï¸ í‘œë¥¼ ì°¾ì§€ ëª»í•¨")
        return []

    print(f"í‘œ ì „ì²´ ê°œìˆ˜: {len(tables)}")

    # í—¤ë” í‚¤ì›Œë“œ
    HEADER_MAP = {
        "ê¸°ì—…ëª…": "name",
        "íšŒì‚¬ëª…": "name",
        "ë²•ì¸ëª…": "name",
        "ìƒí˜¸": "name",
        "Subsidiary": "name",

        "ì—…ì¢…": "business",
        "ì‚¬ì—…": "business",
        "Industry": "business",

        "ì§€ì—­": "region",
        "êµ­ê°€": "region",
        "Country": "region",

        "ì§€ë¶„ìœ¨": "share_ratio",
        "ì§€ë¶„ìœ¨(%)": "share_ratio",
        "Ownership": "share_ratio",
    }

    for idx, tbl in enumerate(tables):
        rows = tbl.find_all(["tr", "TR"])
        if len(rows) < 2:
            continue

        # --------------------------------------------------------
        # 1) í—¤ë” íƒìƒ‰
        # --------------------------------------------------------
        header = None
        header_idx = None

        for i, r in enumerate(rows[:5]):  # ë³´í†µ ìƒìœ„ 5ì¤„ ì•ˆì— header ì¡´ì¬
            cols = r.find_all(["th", "TH", "td", "TD"])
            texts = [c.get_text(" ", strip=True) for c in cols]
            joined = "".join(texts)

            # "ê¸°ì—…ëª…", "ì§€ë¶„ìœ¨" ë“± í•µì‹¬ í—¤ë” í¬í•¨ ì—¬ë¶€
            if any(key in joined for key in HEADER_MAP.keys()):
                header = texts
                header_idx = i
                break

        if header is None:
            continue

        # print(f"âœ” í‘œ {idx}ì—ì„œ header ë°œê²¬: {header}")

        # --------------------------------------------------------
        # 2) header ì´ë¦„ì„ ì •ê·œí™”
        # --------------------------------------------------------
        normalized_headers = []
        for h in header:
            mapped = None
            for key, val in HEADER_MAP.items():
                if key in h:
                    mapped = val
                    break
            normalized_headers.append(mapped if mapped else h)

        # --------------------------------------------------------
        # 3) row ì¶”ì¶œ
        # --------------------------------------------------------
        for r in rows[header_idx + 1:]:
            cols = r.find_all(["td", "TD", "th", "TH"])
            if len(cols) != len(header):
                continue  # irregular row skip

            values = [c.get_text(" ", strip=True) for c in cols]
            row_dict = dict(zip(normalized_headers, values))

            # íšŒì‚¬ëª… ì—†ìœ¼ë©´ ë°ì´í„° ì•„ë‹˜
            if not row_dict.get("name"):
                continue

            subsidiaries.append({
                "name": row_dict.get("name"),
                "region": row_dict.get("region"),
                "business": row_dict.get("business"),
                "share_ratio": row_dict.get("share_ratio"),
                "raw": row_dict
            })

    print(f"âœ” ìµœì¢… ì¶”ì¶œëœ ê³„ì—´íšŒì‚¬ ìˆ˜: {len(subsidiaries)}")
    return subsidiaries

def clean_json_output(text):
    # ì½”ë“œë¸”ëŸ­ ì œê±°
    text = text.strip()
    if text.startswith("```"):
        # ì²« ë²ˆì§¸ ```
        text = text.split("```", 1)[1]
        # ë‘ ë²ˆì§¸ ```
        text = text.split("```", 1)[0]

    # í˜¹ì‹œ "json" ê°™ì€ ì–¸ì–´ íƒœê·¸ ì œê±°
    text = text.replace("json", "", 1).strip()

    return text

def extract_relations_llm(text, corp_name):
    prompt = f"""
ë‹¹ì‹ ì€ ê¸ˆìœµÂ·ì‚°ì—… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ í…ìŠ¤íŠ¸ëŠ” í•œ ê¸°ì—…ì˜ ì‚¬ì—…ë³´ê³ ì„œì´ë©°, ëª©ì ì€ â€œê¸°ì—… ê°„ ê´€ê³„ ê·¸ë˜í”„â€ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì´ ê·¸ë˜í”„ëŠ” GNN ê¸°ë°˜ ë‰´ìŠ¤ ì˜í–¥ ì˜ˆì¸¡ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒì˜ ëª¨ë“  ê´€ê³„ ìœ í˜•ì„ ê°€ëŠ¥í•œ í•œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[1] Supplier(ê³µê¸‰ì—…ì²´)
- raw_supplier
- component_supplier
- manufacturing_outsource

[2] Customer(ê³ ê°ì‚¬)
- major_customer
- b2b_customer
- b2c_channel

[3] Competitor(ê²½ìŸì‚¬)
- direct_competitor
- market_competitor
- potential_competitor

[4] Product/ê¸°ìˆ  í˜‘ë ¥
- tech_partner
- license_in
- license_out
- oem_partner

[5] Governance(ì§€ë°°êµ¬ì¡°)
- subsidiary
- sub_subsidiary
- affiliate
- associate
- joint_venture
- parent
- major_shareholder (ê°œì¸ì€ ì œì™¸)

[6] ê³µê³µê¸°ê´€/ê·œì œ
- regulator
- public_customer
- public_supplier

[7] ê¸ˆìœµ/íˆ¬ì
- loan_provider
- bond_underwriter
- investment_partner

[8] ë¬¼ë¥˜Â·ìœ í†µ
- logistics_partner
- distribution_partner
- retail_channel
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ì—„ê²©í•œ ê·œì¹™:
- ê°œì¸ ì´ë¦„(ì˜ˆ: í™ê¸¸ë™)ì€ ì ˆëŒ€ targetì— í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- targetì€ ë°˜ë“œì‹œ ê¸°ì—…/ë²•ì¸/ê¸°ê´€Â·ë‹¨ì²´ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ë¶ˆí™•ì‹¤í•œ ê´€ê³„ëŠ” ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- sourceëŠ” ë°˜ë“œì‹œ "{corp_name}" ì…ë‹ˆë‹¤.
- JSON ì™¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì¶œë ¥í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
- ì½”ë“œë¸”ë¡(```)ë„ ê¸ˆì§€í•©ë‹ˆë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë”°ë¥´ì‹­ì‹œì˜¤:

{{
  "relations": [
    {{
      "source": "{corp_name}",
      "target": "ê¸°ì—…ëª…",
      "type": "ìœ„ ê´€ê³„ íƒ€ì… ì¤‘ í•˜ë‚˜",
      "evidence": "ì›ë¬¸ ë¬¸ì¥"
    }}
  ]
}}

ì•„ë˜ëŠ” ì‚¬ì—…ë³´ê³ ì„œ ë³¸ë¬¸ì…ë‹ˆë‹¤. ê¸°ì—… ê°„ ê´€ê³„ë§Œ ì¶”ì¶œí•˜ì‹­ì‹œì˜¤.

í…ìŠ¤íŠ¸:
{text}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )

    raw = response.choices[0].message.content

    # ì½”ë“œë¸”ëŸ­ ì œê±°
    cleaned = clean_json_output(raw)

    # JSON íŒŒì‹±
    try:
        return json.loads(cleaned).get("relations", [])
    except Exception as e:
        print("JSON íŒŒì‹± ì‹¤íŒ¨:", cleaned[:500])
        return []

def normalize_name(name):
    return name.replace("(ì£¼)", "").replace("ãˆœ","").strip()

def find_corp_info(name, corp_list):
    norm = normalize_name(name)

    # corp_name ì •ê·œí™”í•œ ì„ì‹œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° í™œìš©
    if "norm_name" not in corp_list.columns:
        corp_list["norm_name"] = corp_list["corp_name"].apply(normalize_name)

    matches = corp_list[corp_list["norm_name"] == norm]

    if matches.empty:
        return None, None  # corp_code, stock_code
    row = matches.iloc[0]
    return row["corp_code"], row["stock_code"]

def extract_text_from_soup(soup):
    return soup.get_text(" ", strip=True)

def chunk_text(text, max_chars=8000):
    chunks = []
    for i in range(0, len(text), max_chars):
        chunks.append(text[i:i+max_chars])
    return chunks

import networkx as nx
from tqdm import tqdm

def build_graph_llm(corp_codes):
    G = nx.DiGraph()

    for corp_code in tqdm(corp_codes):
        print("="*80)
        print(f"ğŸ“Œ corp_code: {corp_code}")
        row = corp_list[corp_list["corp_code"] == corp_code]
        if row.empty:
            continue

        corp_name = row.iloc[0]["corp_name"]

        print(f"íšŒì‚¬ëª…: {corp_name}")
        rcept_no = get_latest_business_report(corp_code)

        print(f"ğŸ“„ rcept_no: {rcept_no}")

        if not rcept_no:
            print("âš ï¸ ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œê°€ ì—†ìŒ, ìŠ¤í‚µ:", corp_name)
            continue

        soup = extract_html_from_document_zip(rcept_no)
        if soup is None:
            print("âš ï¸ ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨, ìŠ¤í‚µ:", corp_name)
            continue

        text = extract_text_from_soup(soup)

        sections = extract_key_sections(text)
        business_text = sections["business"]
        # subsidiary_text = sections["subsidiaries"]

        # ğŸ”¥ ê³„ì—´íšŒì‚¬ table
        subsidiary_rows = extract_subsidiary_table(soup)

        print("ì‚¬ì—…ì˜ ë‚´ìš© ê¸¸ì´:", len(business_text))
        print("í‘œì—ì„œ ì¶”ì¶œí•œ ê³„ì—´íšŒì‚¬ ìˆ˜:", len(subsidiary_rows))

        # ğŸ”¥ ê´€ê³„ ë¬¸ì¥
        rel_sentences = extract_relation_sentences(business_text)

        llm_input = "\n".join(rel_sentences)
        chunks = chunk_text(llm_input, max_chars=8000)

        all_rel = []
        for i, chunk in enumerate(chunks):
            print(f"ğŸ¤– GPT ì²˜ë¦¬ ì¤‘ (chunk {i+1}/{len(chunks)})...")
            r = extract_relations_llm(chunk, corp_name)
            if r:
                all_rel.extend(r)

        print("ğŸ” GPTê°€ ì¶”ì¶œí•œ ì´ ê´€ê³„ ìˆ˜:", len(all_rel))

        edge_set = set()

        # ğŸ”¥ 1) GPT ê´€ê³„ ì¶”ê°€
        for r in all_rel:
            src = r["source"]
            tgt = r["target"]
            rtype = r["type"]

            src_n = normalize_name(src)
            tgt_n = normalize_name(tgt)
            key = (src_n, tgt_n, rtype)

            if key in edge_set: continue
            edge_set.add(key)

            src_corp, src_stock = find_corp_info(src, corp_list)
            tgt_corp, tgt_stock = find_corp_info(tgt, corp_list)

            if (tgt_corp is None) or (tgt_stock is None):
                continue

            G.add_node(src, corp_code=src_corp, stock_code=src_stock)
            G.add_node(tgt, corp_code=tgt_corp, stock_code=tgt_stock)
            G.add_edge(src, tgt, relation=rtype)

        # ğŸ”¥ 2) í…Œì´ë¸” ê¸°ë°˜ ê³„ì—´íšŒì‚¬ ì¶”ê°€
        for row in subsidiary_rows:
            src = corp_name
            tgt = row["name"]

            key = (src, tgt, "subsidiary")
            if key in edge_set:
                continue
            edge_set.add(key)

            src_c, src_s = find_corp_info(src, corp_list)
            tgt_c, tgt_s = find_corp_info(tgt, corp_list)

            G.add_node(src, corp_code=src_c, stock_code=src_s)
            G.add_node(tgt, corp_code=tgt_c, stock_code=tgt_s, region=row["region"], business=row["business"])

            G.add_edge(src, tgt, relation="subsidiary", share=row["share_ratio"])


    return G

from torch_geometric.data import Data

def graph_to_pyg(G, embedding_dim=128):
    idx = {node: i for i, node in enumerate(G.nodes())}

    # ì„ì‹œ ì„ë² ë”© (GPU ë©”ëª¨ë¦¬ ì ˆì•½ ëª©ì )
    x = torch.randn((len(G.nodes()), embedding_dim))

    edges = []
    for src, dst in G.edges():
        edges.append([idx[src], idx[dst]])
    if len(edges) == 0:
        # ë¹ˆ ê·¸ë˜í”„ ì²˜ë¦¬: ë¹ˆ edge_index
        edge_index = torch.empty((2, 0), dtype=torch.long)
    else:
        edge_index = torch.tensor(edges).t().contiguous()

    data = Data(x=x, edge_index=edge_index)
    return data

import pandas as pd

# 1) CSV ë¡œë“œ
theme_df = pd.read_csv("theme_stock.csv")

def extract_stock_names(row):
    if pd.isnull(row):
        return []
    return [x.strip() for x in str(row).replace("\t", ",").replace(" ", ",").split(",") if x.strip()]

# 2) ì¢…ëª© ëª©ë¡ ì „ì²´ ì¶”ì¶œ
stock_names = []
for items in theme_df["ì¢…ëª© ëª©ë¡"]:
    stock_names.extend(extract_stock_names(items))
stock_names = list(set(stock_names))
print("ì´ ì¢…ëª© ìˆ˜:", len(stock_names))
print(stock_names)

# 3) corp_listì—ì„œ corp_code ë§¤ì¹­
def get_corp_codes_from_names(stock_names, corp_list):
    codes = []
    for name in stock_names:
        match = corp_list[corp_list["corp_name"].str.contains(name, na=False)]
        if not match.empty:
            codes.extend(list(match["corp_code"]))
        else:
            print(f"âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨: {name}")
    return list(set(codes))

filtered_corp_codes = get_corp_codes_from_names(stock_names, corp_list)

print("í•„í„°ë§ëœ corp_code ìˆ˜:", len(filtered_corp_codes))
print(filtered_corp_codes)


# 4) ê·¸ë˜í”„ ìƒì„±
G = build_graph_llm(filtered_corp_codes)

print("Nodes:", G.number_of_nodes())
print("Edges:", G.number_of_edges())

# ì €ì¥ ë””ë ‰í† ë¦¬: ê¸°ë³¸ ë¡œì»¬ output ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ SAVE_DIR ì‚¬ìš©
save_dir = os.getenv("SAVE_DIR", "output")
os.makedirs(save_dir, exist_ok=True)
print("ì €ì¥ í´ë”:", save_dir)

# ì˜ˆì‹œ ì €ì¥ (ì‹¤ì œ ì‹¤í–‰ ì‹œ ì£¼ì„ í•´ì œ)
data = graph_to_pyg(G)
with open(f"{save_dir}/relationship_graph_llm.pkl", "wb") as f:
    pickle.dump(G, f)
torch.save(data, f"{save_dir}/relationship_graph_pyg_llm.pt")
